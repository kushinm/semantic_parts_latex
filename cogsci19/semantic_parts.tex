% 
% Annual Cognitive Science Conference
% Sample LaTeX Paper -- Proceedings Format
% 

% Original : Ashwin Ram (ashwin@cc.gatech.edu)       04/01/1994
% Modified : Johanna Moore (jmoore@cs.pitt.edu)      03/17/1995
% Modified : David Noelle (noelle@ucsd.edu)          03/15/1996
% Modified : Pat Langley (langley@cs.stanford.edu)   01/26/1997
% Latex2e corrections by Ramin Charles Nakisa        01/28/1997 
% Modified : Tina Eliassi-Rad (eliassi@cs.wisc.edu)  01/31/1998
% Modified : Trisha Yannuzzi (trisha@ircs.upenn.edu) 12/28/1999 (in process)
% Modified : Mary Ellen Foster (M.E.Foster@ed.ac.uk) 12/11/2000
% Modified : Ken Forbus                              01/23/2004
% Modified : Eli M. Silk (esilk@pitt.edu)            05/24/2005
% Modified : Niels Taatgen (taatgen@cmu.edu)         10/24/2006
% Modified : David Noelle (dnoelle@ucmerced.edu)     11/19/2014

%% Change "letterpaper" in the following line to "a4paper" if you must.

\documentclass[10pt,letterpaper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{apacite}


\title{semantic parts}
 
\author{{\large \bf Judy} \AND {\large \bf Robert}\AND {\large \bf Kushin}
  }




\begin{document}

\maketitle 


\begin{abstract}
This is where our abstract will go

\textbf{Keywords:} 
sketching; cognitive science; perception
\end{abstract}



\section{Introduction}
This is where our introduction will go




\section{Methods}


\subsection{Stimuli}

 From \_\_ we obtained 1198 sketches of 32 real-world objects belonging to 4 basic-level categories : cars, chairs, dogs, and birds. There were 8 diverse exemplars per category.
The sketches were made during a 2-player ‘Pictionary’-style game, where a sketcher had to create sketches of target images so as to distinguish the target from 3 distractor images. The viewer had to guess which of the 4 images the sketch represented.
There were 2 main context conditions in the original experiment - close and far. In the close condition the target image and the distractors belonged to the same basic-level category. In the far condition, the target and each of the distractors belonged to a different basic-level category.

The sketches were represented as scalable vector graphics (SVG) images. We were interested in having participants label strokes in a given sketch. We defined a stroke to be equivalent to a single cubic Bézier curve, i.e., a Bézier curve with two fixed end points and two control points to control curvature.



\subsection{Participants}

We recruited a total of 326 participants via Amazon Mechanical Turk (AMT). Participants were paid a base amount of \$1 and were given an additional bonus of \$0.002 for every stroke they annotated. In addition to this, they were given a \$0.02 bonus for every sketch for which they labeled all strokes. 


\subsection{Annotation Procedure}

To collect fine-grained annotations of our sketches, we implemented a web-based Javascript annotation tool. 
Our task consisted of one demonstration trial and 10 annotation trials. We provided participants with a sketch to be annotated on a canvas as well as a category-specific menu of labels, which they were encouraged to use for the annotation task. We also provided them with the option of entering their own labels through a free-response box, which could be accessed by clicking an option called ‘Other’ on the menu. 
The participant could see the constituent strokes of the sketch by hovering their cursor over different parts of the canvas.  To label a stroke, they could click on it or click and drag their cursor over several strokes. This would highlight the selected strokes. Next, they would choose a label from the menu or submit their own through the free response box. The highlighted strokes would change color to match the background color of the selected part-word in the menu. This was to provide a visual aid to the participant as to what strokes had already been labeled and as what part. Participants were encouraged to conduct their labeling of strokes in bouts — they were to highlight all the strokes corresponding to a single instance of a part before selecting a label from the menu. A running counter next to each part would indicate how many bouts of annotation had been completed for that part. A running progress meter below the canvas would inform the sketcher as to how many strokes they had labeled out of the total number of strokes in the sketch. Once they were done with sketch, participants could go on to the next trial by clicking a ‘Next’ button under the canvas. They could choose to continue to the next trial without labeling every stroke in a sketch, but they would lose out on the completion bonus as well as the amount they would have earned for labeling the remaining strokes.
After receiving instructions on how to use the tool, participants were told to test out the interface in a demonstration trial. For this trial alone they could not proceed unless they labeled every stroke in the sketch. The sketch shown for the demo trial was the same for every participant.

\noindent In total we collected 3608 annotations


\subsection{Analysis}


\section{Results}


\section{Discussion}


\section{Acknowledgments}


\subsection{Tables}


\subsection{Figures}






\section{References}

\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}

\bibliography{CogSci_Template}


\end{document}

% 
% Annual Cognitive Science Conference
% Sample LaTeX Paper -- Proceedings Format
% 

% Original : Ashwin Ram (ashwin@cc.gatech.edu)       04/01/1994
% Modified : Johanna Moore (jmoore@cs.pitt.edu)      03/17/1995
% Modified : David Noelle (noelle@ucsd.edu)          03/15/1996
% Modified : Pat Langley (langley@cs.stanford.edu)   01/26/1997
% Latex2e corrections by Ramin Charles Nakisa        01/28/1997 
% Modified : Tina Eliassi-Rad (eliassi@cs.wisc.edu)  01/31/1998
% Modified : Trisha Yannuzzi (trisha@ircs.upenn.edu) 12/28/1999 (in process)
% Modified : Mary Ellen Foster (M.E.Foster@ed.ac.uk) 12/11/2000
% Modified : Ken Forbus                              01/23/2004
% Modified : Eli M. Silk (esilk@pitt.edu)            05/24/2005
% Modified : Niels Taatgen (taatgen@cmu.edu)         10/24/2006
% Modified : David Noelle (dnoelle@ucmerced.edu)     11/19/2014

%% Change "letterpaper" in the following line to "a4paper" if you must.

\documentclass[10pt,letterpaper]{article}

\usepackage{cogsci}
\usepackage{comment}
\usepackage{pslatex}
\usepackage{apacite}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{color}
\usepackage{url}
\usepackage{todonotes}
\usepackage{mathtools}
\usepackage{stmaryrd}
\usepackage{booktabs}
\usepackage{array}
\usepackage{caption}
\usepackage{sidecap}
\usepackage{capt-of}
\usepackage{nicefrac}
\usepackage[export]{adjustbox}
\usepackage{makecell}
\hyphenpenalty=100000
\renewcommand\theadfont{\normalsize}
\usepackage[font={footnotesize}]{caption}
\newcommand{\jefan}[1]{{\color{blue}{[jefan: #1]}}}
\newcommand{\kushin}[1]{{\color{orange}{[kushin: #1]}}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}


\title{Semantic structure in communicative sketches}

 
% \author{\begin{tabular}[htbp]{c@{\extracolsep{1em}}c@{\extracolsep{1em}}c@{\extracolsep{1em}}c} \\
% {\large \bf Kushin Mukherjee} & {\large \bf Robert X. D. Hawkins} & {\large \bf Judith E. Fan}\\
% Department of Cognitive Science  & Department of Psychology & Department of Psychology \\ 
% Vassar College & Stanford University & Stanford University \\
% \texttt{kumukherjee@vassar.edu} & \texttt{rxdh@stanford.edu} & \texttt{jefan@stanford.edu} \\
% \end{tabular}
% }

% \author{{\large \bf Kushin Mukherjee\textsuperscript{1}, Robert X. D. Hawkins\textsuperscript{2}, Judith E. Fan\textsuperscript{2,3}} \\
% \textsuperscript{1}Department of Cognitive Science, Vassar College, \\
% \textsuperscript{2}Department of Psychology, Stanford University, \\
% \textsuperscript{3}Department of Psychology, University of California, San Diego}

\author{\large \bf Anonymous Authors}

\begin{document}
\makeatletter
\let\@oldmaketitle\@maketitle% Store \@maketitle
\renewcommand{\@maketitle}{\@oldmaketitle% Update \@maketitle to insert...
  \includegraphics[width=0.95\textwidth]
    {figures/1_banner.pdf}
    \captionsetup{width=0.95\textwidth}
 \captionof{figure}{\footnotesize{Objects used in communication game with example sketches of each object, where stroke color indicates different parts.}}\bigskip}% ... an image
\makeatother

\maketitle 

\begin{abstract}

The ability to represent semantically meaningful structure in our environment is a powerful aspect of human visual perception and cognition. 
As a testament to this ability, we effortlessly grasp the correspondence between a sketch of an object and that physical object in the world, even if the sketch is far from realistic. 
How are visual object concepts organized such that they can robustly encode such abstract correspondences?
Here we consider the possibility that this is in part because we readily decompose both objects and sketches into a common set of semantically meaningful parts. 
To evaluate this, we developed a web-based platform to densely annotate the object parts represented in sketches of real-world objects that varied in how detailed they were.  
We found that: \textit{1}, people are highly consistent in how they interpret what individual strokes represent; \textit{2}, single strokes tend to correspond to single parts; \textit{3}, strokes representing the same part tend to be clustered in time during production; and \textit{4}, both more detailed and sparser sketches of the same object emphasized similar part information, although \textit{5}, detailed sketches of different objects tend to be more distinct from one another than sparser ones. 
Taken together, our results support the notion that people deploy their abstract understanding of the compositional part structure of objects in order to select actions to communicate relevant information about them in context. 
More broadly, they highlight the importance of structured knowledge for understanding how pictorial representations convey meaning. 

\textbf{Keywords:} 
sketch understanding; perceptual organization; visual production; compositionality; objects and categories
\end{abstract}

\section{Introduction}

%% revise so that the throughline is clearer: how does the human mind organize visual concepts such that they can be deployed so flexibly? answer: compositionality. what is a good way of probing compositionality? you could use discrimination tasks, but compositional tasks are much more direct. what is good about compositionality? it enables flexibility. how do you probe flexibility? compositional tasks across different contexts. 

When we open our eyes, we do not experience a meaningless array of photons --- instead, we parse the world into people, objects, and their relationships. 
The ability to represent semantically meaningful structure in our environment is a core aspect of human visual perception and cognition \cite{navon1977forest}. 
As a testament to this ability, we effortlessly grasp the correspondence between a sketch of a particular object and that physical object in the world, even if the sketch is far from realistic \cite{eitz2012humans,FanCommon2018}. 
How are visual object concepts organized such that they can robustly encode such abstract correspondences?
Here we explore the hypothesis that this is in part because we readily decompose both objects and sketches into a common set of semantically meaningful parts \cite{biederman1988surface}. 

Recent advances in computational neuroscience and artificial intelligence have provided an unprecedentedly clear view into the algorithms used by the brain to extract semantic information from raw visual inputs, exemplified by modern deep learning approaches \cite{yamins2014performance}.
Nevertheless, a major gap remains in elucidating how the feature representations learned by deep learning models can be adapted to emulate the structure and flexibility of human visual semantic knowledge \cite{lake2017building}.
A promising approach to closing this gap may be to combine the learning capacity of deep neural networks with the parsimony and interpretability of structured representations that reflect how visual concepts are organized in the mind \cite{battaglia2018relational}. 
However, pursuing this strategy relies on a thorough understanding of this conceptual organization and how this organization enables behavioral flexibility.  

The goal of this paper is to contribute to this understanding by probing the expression of visual semantic knowledge in a naturalistic setting that exposes both its structure and flexibility: visual communication via drawing. 
This approach departs from the conventional strategy for inferring the organization of visual object concepts from behavior, which relies upon tasks that elicit judgments about visual inputs, usually with respect to experimenter-defined dimensions. 
By contrast, visual communication tasks permit participants to include any elements they consider relevant to their goals and combine these elements freely, yielding high-dimensional information about how visual semantic knowledge is organized and deployed under a naturalistic task objective. 

Our aim in probing the semantic structure of communicative sketches is to shed light on how the semantic organization of visual object representations supports their flexible expression across contexts. 
Our approach advances recent work \cite{FanCommon2018} that has investigated the production of object sketches to communicate in two ways: first, an explicit focus on compositional semantic structure in sketches, and second, the examination of flexibility in how visual semantic knowledge is expressed in different semantic contexts. 

Towards this end, we developed a web-based platform to densely annotate sketches of real-world objects produced in different semantic contexts, including detailed and simpler sketches of each object. 
Overall, we found that: (1) people are highly consistent in how they interpret what individual strokes represent; (2) single strokes tend to correspond to single parts; (3) strokes representing the same part tend to be clustered in time; and (4) detailed and sparse sketches of the same object emphasized similar part information, although (5) detailed sketches of different objects tend to be more distinct from one another than simpler ones. 
Taken together, our results support the notion that people deploy their abstract understanding of the compositional part structure of objects in order to select actions to communicate relevant information about them in context. 

\section{Methods}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{figures/2_refgame_performance.pdf}
\caption{(A) Sketches were collected in the context of a two-player drawing-based reference game in which one participant (sketcher) aimed to draw a target object so that the other participant (viewer) could distinguish it from three distractor objects. (B) In close contexts, the target and distractors all belonged to the same basic-level category; in far contexts, the target and distractors belonged to different basic-level categories. (C) Sketchers used fewer strokes in the far condition, while producing sketches that were accurately recognized by the viewer in both conditions.}
\label{refgame_performance}
\end{figure}

\subsection{Communicative sketch dataset}
We obtained 1195 sketches of 32 real-world objects from a recent experimental dataset in which participants were paired in an online environment to play a drawing-based reference game \cite{fan2018modeling}.\footnote{All materials and data will be made available following the completion of the review process.}
% \footnote{All materials and data are available at \url{https://github.com/cogtoolslab/semantic_parts}.}
Objects belonged to one of four basic-level categories (i.e., bird, car, chair, dog), each of which contained eight exemplars. %(Fig.~\ref{refgame_gallery}A).
On each trial of this reference-game experiment, both participants were presented with a shared context containing an array of photorealistic 3D renderings of four objects.   
One participant (i.e., the sketcher) aimed to draw one of these objects -- the target -- so that the other participant (i.e., the viewer) could pick it out from a set of distractor objects (Fig.~\ref{refgame_performance}A). 
Across trials, the similarity of the distractors to the target was manipulated, yielding two types of communicative contexts: close contexts, where the target and distractors all belonged to the same basic-level category, and far contexts, where the target and distractors belonged to different basic-level categories (Fig.~\ref{refgame_performance}B). 
This context manipulation led sketchers to produce simpler sketches containing fewer strokes and less ink on far trials than on close trials, while still achieving high recognition accuracy in both types of context (Fig.~\ref{refgame_performance}C). %, Fig.~\ref{refgame_gallery}B\&C). 
 
% \begin{figure*}
% \centering
% \includegraphics[width=0.95\textwidth]{figures/3_refgame_gallery.pdf}
% \caption{(A) Target objects. (B) Example object sketches produced in a close context. (C) Example sketches produced in a far context.}
% \label{refgame_gallery}
% \end{figure*}


Prior work analyzing the semantic properties of such sketch data have represented them as raster images (e.g., \texttt{*.png}), an expedient format for applying modern convolutional neural network architectures \cite{FanCommon2018,sangkloy2016sketchy,yu2017sketch}. 
However, a key limitation of representing a sketch only as an image is loss of information about the inherently sequential and contour-based nature of sketch production. 
Because our goal was to characterize how semantic structure manifests during sketch production, it was important to encode each sketch instead using a vector image format that preerves this sequential and contour information (i.e., \texttt{*.svg}). 

Each sketch in our dataset is represented as a sequence of individual strokes, where each stroke consists of a sequence of sub-stroke elements, known as splines. 
These splines are parameterized as cubic Bezier curve segments, which are uniquely defined by four points: the initial point, the final point, and two control points that control the spline's curvature.
This data format provides a relatively compact representation of each sketch compared with a rasterized image, while still providing sufficient expressivity to provide an accurate representation. 

\subsection{Semantic part annotation}

Here we developed a novel web-based platform to crowdsource semantic annotations for every spline of every stroke of communicative sketches in our dataset. 

\subsubsection{Participants}
326 participants were recruited via Amazon Mechanical Turk (AMT) and provided informed consent in accordance with the Stanford University IRB. 
Participants were provided with a base compensation of \$0.35, plus \$0.002 for every spline they annotated and \$0.02 for every sketch they annotated completely. 

\subsubsection{Task}
Participants were presented with 10 sketches that were randomly sampled from the reference-game dataset. 
Each trial, one of these sketches appeared in the center of the display, above the same array of four objects that the original sketcher had viewed, with one of these objects highlighted as the target (Fig.~\ref{annotation_interface}). 
Thus the participant had full information about which object the sketcher had intended to depict, as well as the identity of the distractors. 
Their goal was to tag each spline with a label corresponding to the part it represented (e.g., seat, leg, back for a chair). 
To facilitate this, participants were provided with a menu of common part labels that were associated with each basic-level category represented in our dataset. 
Once a spline was successfully tagged, it was rendered in the color that the part label was associated with in the part menu. 
However, participants could also generate their own part label if none of the common labels applied.
In total, we collected 3608 annotation trials of 1195 unique sketches.

\subsubsection{Data preprocessing}

To avoid bias when analyzing the relative emphasis that sketchers placed on different part information, we restricted our analyses to annotation trials in which the sketch was completely annotated (i.e., all splines were tagged). 
Moreover, to evaluate inter-annotator reliability, we only examined sketches that were annotated by at least three distinct participants. 
Some of the custom part labels provided by participants were valid, but at a finer grain than or synonymous to other more frequently occurring labels. 
For example, sometimes strokes that represented subparts of the leg of a chair were labeled as `leg support', `foot', and `strut.'
In order to ensure that parts of sketches were segmented at a consistent level of granularity, we manually constructed a part dictionary to map these overly fine-grained part labels to one of the common part labels. 
After applying these preprocessing steps, our annotated dataset consisted of 864 sketches that had been annotated exactly 3 times each, using a set of 24 unique part labels. 

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{figures/4_annotation_interface.pdf}
\caption{Sketch annotation interface. Participants selected sub-stroke elements of the sketch and tagged them with category-congruent part labels from the menu. The original communicative context in which the sketch was generated was also provided: the target object (highlighted in red) and three distractors were displayed in an array below the sketch.}
\label{annotation_interface}
\end{figure}

\begin{table}[]
\begin{tabular}{ll}
\hline
\textbf{category} & \textbf{part labels} \\ \hline
bird & eye, beak, head, body, wing, leg, feet, tail \\ \hline
car & \begin{tabular}[c]{@{}l@{}}bumper, headlight, hood, windshield, \\ window, body, door, trunk, wheel\end{tabular} \\ \hline
chair & backrest, armrest, seat, leg \\ \hline
dog & eye, mouth, ear, head, neck, body, leg, paw, tail \\ \hline
\end{tabular}
\caption{Part labels provided to annotators. All user-generated labels were mapped to this set of labels.}
\end{table}


\section{Results}

\subsection{How well do different people agree on what strokes represent?}

Perhaps one of the most basic questions our dataset is poised to answer is how often different viewers agreed on what each stroke element in a sketch represented. 
We found that 95.6\% of all splines in our dataset were given the same label by at least two of the three annotators, with 67.8\% of all splines exhibiting complete agreement among all three annotators. 
This suggests that there is a high degree of consistency in how people decompose sketches into semantically meaningful parts. 
In subsequent analyses, we collapsed over inter-annotator variation: we assigned the modal label to splines to which at least two annotators had given the same label; for the remaining 4.4\% of splines, we sampled one of the three labels provided.

\subsection{How strongly do strokes correspond to parts of objects?}

When composing a recognizable sketch of a real-world object, how do people decide what information to convey with each stroke? 
A natural possibility is that there exists a close correspondence between the actions they perform and what parts they know the object possesses. 
Concretely, we hypothesized that for most strokes in our dataset, that all sub-stroke elements (splines) would be assigned the same part label. 
However, because the part labels we considered could reasonably apply to multiple sub-parts (e.g., multiple legs on a bird, chair, or dog), we further hypothesized that multiple strokes would frequently be used to represent all instances of that part in a single sketch.

To evaluate the first prediction, we determined how many unique part labels were represented across all sub-stroke elements within each stroke (Fig.~\ref{stroke_to_part}B). 
We found that there was only one part label associated with 81.6\% of the strokes in our dataset, showing that when participants produced a stroke, most of the time it represented no more than one part of that object. 
18.4\% of strokes were associated with two or more labels, showing that there are still cases where a single stroke may still be used to represent more than one part (e.g., a single stroke connecting the head and body of a bird, or an armrest and leg of a chair). 
This was slightly more common in sketches produced in far contexts (19.4\%, 95 \%CI = 17.9\%, 20.9 \%) than close contexts (17.6\%, 95\% CI : [16.1\%, 18.8\%]).  This indicates that a candidate strategy for creating sparser abstract sketches may be to utilize individual strokes to represent multiple spatially contiguous parts. 

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{figures/5_stroke_part_relationship.pdf}
\caption{Correspondence between strokes and part labels. (A) We examined the number of unique labels assigned to the constituent splines of every stroke in our dataset. We also counted how many strokes shared a given part label in every sketch. (B) The majority of strokes contained only a single part label. (C) Almost half the parts across all sketches required only a single stroke to be represented. The remaining parts required at least 2 strokes to be fully represented. }
\label{stroke_to_part}
\end{figure}

To evaluate the second prediction, we determined how many strokes were used to represent each part of an object (Fig.~\ref{stroke_to_part}B). 
We found that 46.1\% of parts were depicted using exactly one stroke, 26.0\% using exactly two strokes, 11.3\% using exactly three strokes, 16.6\% using four or more strokes. 
This shows that nearly half the time, a single action was required to depict an entire object part. 
However, the remaining 53.9\% of parts required more than one stroke to depict, owing to those parts that consisted of multiple disconnected subparts within an object (e.g., wheels of a car, paws of a dog).
\jefan{This should be verified in some way by subsetting on those feature columns that actually are likely to contain multiple instances per object.}
The proportion of parts requiring more than one stroke was slightly higher for close sketches (55.8\%, 95\% CI: [53.7\%, 58.6\%]) than far sketches (52.0\%, 95\% CI: [49.9\%, 54.6\%]), perhaps reflecting the tendency to include more object-diagnostic detail when depicting each part. 

Overall, these findings suggest that when people have the goal of communicating the identity of an object in a drawing, the information they choose to convey with each stroke is far from arbitrary, but rather is tightly coupled to the compositional part structure of objects. 

\subsection{How often are strokes representing the same part produced in succession?}

In the previous section we discovered that for slightly more than half of the parts in each object, more than one stroke was used to depict it. 
This raised the question: to what extent are these multiple strokes depicting the same part being drawn in succession, or being interleaved among strokes corresponding to other parts?

%To investigate this, we estimated the mean length of part streaks consisting of strokes depicting the same part, and compared this to the mean streak length for permuted stroke sequences. 
%We used mean streak length as a proxy for the prevalence of same-part strokes drawn in succession. 
To investigate this, we computed for each sketch its part streak lengths — counts of the number of successive strokes depicting the same part drawn in succession — and compared the mean of these streak lengths to the mean streak length for permuted stroke sequences. 
To compute the mean steak length, we encoded each sketch as a sequence of strokes, where each stroke was represented by the modal part label assigned to its sub-stroke elements. 
We restricted this analysis to sketches that consisted of more than one stroke, consisted of more than one part label, and contained at least two strokes sharing the same part label (78 out of 864 sketches excluded). 
For every element of the stroke sequence, we recorded the length of the part streak it belonged to. 
If it was immediately followed by a stroke depicting a different part, the streak length would be 1. 
If it was immediately followed by two strokes depicting the same part, before switching to a stroke of a different part, the streak length would be 3. 
We then averaged these streak length values over the entire sequence. 
To evaluate to what extent these mean streak lengths exceeded that expected under a null model in which there was no tendency for same-part strokes to be clustered in time, we compared our empirical estimates of streak length to a null distribution of streak lengths generated by permuting each stroke sequence 1000 times (Fig.~\ref{stroke_sequence_fig}A). 

\begin{figure}[ht]
\centering
\includegraphics[width=0.45\textwidth]{figures/6_part_sequence.pdf}
\caption{(A) Analysis of sequence in which strokes depicting each part were drawn. (B) Comparison of mean length of streaks consisting of strokes that depict the same part with null distribution of permuted stroke sequences.}
\label{stroke_sequence_fig}
\end{figure}


To evaluate statistical reliability, we computed the z-score of the empirical streak length relative to the permuted streak length distribution for each sketch. 
This reflects the extent to which same-part strokes were clustered in time relative to a sequence with exactly the same distribution of strokes over parts, but in which strokes were produced in random order. If the empirical streak lengths were to be higher than the permuted streak lengths, it would indicate that not only do people's strokes tend to correspond to semantically meaningful parts, but that the sequence in which they make strokes to create sketches also reflects their knowledge of object-specific part structure. 
We found that the empirical streak length was reliably higher for all objects (z-score: 2.07, 95\% CI: [1.90, 2.23]), and higher for the close sketches (z-score: 2.58; 95\% CI: [2.26, 2.90]) than far sketches (z-score: 1.56; 95\% CI: [1.38, 1.74]).
Since close sketches tend to comprise of more strokes than far sketches, it is possible that these additional strokes serve to provide additional detail to parts in the sketch. If so, then parts are constructed at a finer compositional grain in close sketches \kushin{might work on wording here; feel like there's a missing link}
leading to a larger number of successive strokes of the same part. Our results support this idea.
All confidence intervals were estimated via stratified bootstrap resampling (N=1000 iterations) of sketches within object-context combinations.
Taken together, our results suggest that the procedure by which people convey semantic information when sketching is compositional: frequently, they use a single stroke to convey an entire part; but when they do use multiple strokes to convey a single part, they tend to draw these in succession before moving on to a different part. 

\subsection{How is part information emphasized in different communicative contexts?}

Our findings so far bear on how the way people compose communicative sketches of objects reflects their semantic knowledge of which parts objects are composed of. 
A key consequence of such semantically organized part knowledge is that it naturally supports flexible expression across different communicative contexts. 
For example, when communicating about a chair in a (close) context containing objects from other basic-level categories, sketchers may include only the essential information to indicate the presence of certain parts that distinguish it at the category level. 
On the other hand, when communicating about that same chair in a (far) context containing other, perceptually similar chairs, sketchers may emphasize certain distinctive parts (e.g., the number of back slats) that distinguish it at the object level, by applying more strokes and/or more ink in each stroke.

We hypothesized that sketchers emphasize part information across such communicative contexts in such a way that it preserves these relevant distinctions. 
To explore this possibility, we asked the following questions: 
(1) How similarly is object-specific part information emphasized in both close and far communicative contexts? 
(2) To the extent close and far sketches of the same object emphasize part information differently, does this reflect large differences in emphasis on a few parts, or small differences in emphasis across many parts? 
(3) What are the consequences of the way sketchers modulate their emphasis across parts on the overall distinctiveness of close and far sketches?  To investigate these questions, we represented each sketch by a \textit{part-feature vector} that combined information about: (a) how many strokes and (b) how much total ink was expended on each part of that object. 
In order to represent all sketches in our dataset using a common feature representation, we combined part labels across categories, yielding a set of 24 unique part labels to which any stroke could be assigned. 
Each part-feature vector thus consisted of 48 elements: 24 of these represented the number of strokes allocated to each part, and the remaining 24 represented the total arc length of all strokes allocated to each part. 
Before further analysis, we z-scored values within each feature dimension in order to map stroke count and arc length measurements to the same unit-variance scale. 
Because our primary goal was to understand differences between objects and contexts, we then collapsed across sketches within each object-context combination, yielding 64 average part-feature vectors (i.e., 32 objects x 2 context conditions). 

\begin{figure}[ht]
\centering
\includegraphics[width=0.47\textwidth]{figures/7_part_emphasis.pdf}
\caption{(A) Multidimensional scaling of the correlation distance between objects. Generally, objects belonging to the same category-condition block appear to cluster closer together.  (B) Part profiles of close and far sketches of the same object were more similar to each other than those of close and far sketches of different objects within a category.  (C) Moderate vector sparsity between context conditions indicating that all parts are not equally emphasized in close sketches relative to their far counterparts.(D) Far sketches were more similar to each other than close sketches were to each other in terms of part features. \jefan{need to add bars and stars, also other annotation to help read which end of sparsity plot is maximal.} }
\label{part_emphasis}
\end{figure}

\subsubsection{Similar part profile across different communicative contexts}

In order to investigate to what extent similar object-specific part information is emphasized in different communicative contexts, we computed the matrix of Pearson correlations between part-feature vectors. 
Formally, this entailed computing: $(R)_{ij} =  \frac{cov(\vec{r}_{i}, \vec{r}_{j})}{\sqrt{var(\vec{r}_{i}) \cdot var(\vec{r}_{j})}}$, where $\vec{r}_{i}$ and $\vec{r}_{j}$ are the mean part-feature vectors for the $i$th and $j$th object-context combinations, respectively.
If similar object-specific part information is emphasized in both contexts, this would yield higher correlations between close and far part-feature vectors for the \textit{same} object than for close and far part-feature vectors of \textit{different} objects. 
Consistent with this, we found that close and far sketches of the same object emphasized similar part information (within-object: $r = 0.740$, 95\% CI: [0.726, 0.753]), and to a greater degree than did close and far sketches of different objects (between-object: $r = 0.653$, 95\% CI: [0.646, 0.659]), suggesting that these patterns of emphasis were to some extent object-specific (Fig.~\ref{part_emphasis}B). 

\subsubsection{Greater emphasis on a few parts when producing detailed sketches}

Although the patterns of emphasis on different parts were similar between close and far sketches, close sketches expressed more part information overall.
To what extent do close sketches accomplish this by placing a lot more emphasis on a few parts, or placing a little extra emphasis on many parts?
To address this question, we first calculated the vector difference between the close and far part-feature vectors for each object.
% To provide a geometric intuition, each difference vector represents the direction and distance to move in this 48-dimensional feature space to get from the centroid of the far sketches of an object to the centroid of its close sketches. 
If close sketches place a lot more emphasis on a few parts, this difference vector will be sparse, containing a few large non-zero values and many zero values. 
On the other hand, if close sketches tend to place a little extra emphasis on many parts, this difference vector will not be non-sparse, containing many similar non-zero values and few zero values. 
We used the following metric to compute the sparsity of each difference vector:
$\texttt{s}= \nicefrac{(\sqrt{k} - \frac{\norm{v}_1}{\norm{v}_2})}{(\sqrt{k}-1)}$, where $k=48$ (the dimensionality of the vector), $v$ is the difference vector, $\norm{v}_1$ is the L1-norm and $\norm{v}_2$ is the L2-norm. 
Here, $s=1$ for maximally sparse vectors (i.e., single non-zero value) $s=0$ for minimally sparse vectors (i.e., all values identical and non-zero).
We found that difference vectors were moderately sparse ($s=0.675$, 95\% CI: [0.653, 0.686]), suggesting that close sketches place more emphasis on a few different parts relative to far sketches, but not to the same extent for all parts (Fig.~\ref{part_emphasis}C).

\subsubsection{Detailed sketches are more distinct from each other than sparser sketches}

Given the above findings that close sketches exhibited similar patterns of emphasis on different parts as their far counterparts, but with greater emphasis on a few of these in particular, a natural question arises concerning the functional significance of this form of contextual modulation. 
We hypothesized that such modulation enhances the overall discriminability of close sketches, relative to far ones, by increasing the feature distance between objects that otherwise share many perceptual properties. 
To evaluate this possibility, we compared the mean correlation distance (i.e., $1 - r$) between the part-feature vectors of close sketches of objects in a given category with that between far sketches of exactly the same objects. 
We found that close sketches were indeed reliably more distant from one another than far sketches were (close similarity: $r = 0.666$, 95\% CI: [0.652,685]; far similarity: $r = 0.729$, 95\% CI: [0.719,0.746]), suggesting that sketchers discern which part information is most diagnostic of the target object among highly similar distractors, and emphasize this distinctive information accordingly (Fig.~\ref{part_emphasis}A \& D).

\section{Discussion}


Humans are able to leverage their abstract understanding of objects to equivalently recognize them across a variety of modalities — photographs, 3D renders, sculptures, and real world instances. A testament to the robustness of this ability is that even with limited information, as is provided by the sparse sketches in our reference game dataset, people are deftly able to perform this task of recognition. We show that this abstract understanding even extends to the realm of graphical production, allowing people to effectively communicate with each other using only a limited number of strokes on a canvas.

Our results support the idea that this understanding is, in part, facilitated by people's semantic knowledge, namely their knowledge of the compositional nature of category-specific parts. 
%The prevalent one-to-one correspondences between strokes made by the sketcher and part-labels assigned to those strokes by annotators indicate that this abstract knowledge is deployed for both graphical production and recognition.
This notion is also supported by the fact that in the eyes of our sketch annotators, sparse and detailed sketches exhibit similar part information with detailed sketches emphasizing certain diagnostic parts over others. This flexibility of expression across varied communicative contexts is a powerful aspect of such part-based representations.

While our findings show that people's semantic knowledge of parts may influence how they recognize and produce sketches, we believe our current part-feature characterization to be an overly explicit instantiation of semantics in communicative sketches. Though we believe such approximations are beneficial to showing how modulating semantically meaningful information in sketches allows for flexibility in communication, we are yet to account for how perceptual properties of strokes in sketches are perceived as components that hold meaning for us. Thus, we are still shy of being able to translate the high dimensional feature spaces in modern vision models to more human-like perceptual representations through this approach alone. Future work will seek to integrate these two approaches.
\kushin{More elaboration on the benefits of the approaches we apply in this paper. Maybe the last sentence or two aren't required.}


\jefan{a thought to be fleshed out more: all the more remarkable we can extract these things based purely on considering something like effort cost // number of actions across categories. Future work should focus on extending this approach to consider the \textit{content} allocated to each part category, over and above \textit{quantity.}}
 

\section{Acknowledgments}
%Hide for now since blinded

%KM was supported by the Department of Cognitive Science at Vassar College through its Humanities in Cognitive Science program.
%RXDH was supported by the National Science Foundation Graduate Research Fellowship (DGE-114747). 

%\section{References}

\bibliography{semantic_parts}
\bibliographystyle{apacite}
\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}ı






\end{document}
